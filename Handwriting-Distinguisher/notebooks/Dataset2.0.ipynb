{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e9c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de4356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=10):\n",
    "        \"\"\"\n",
    "        Define the layers of the convolutional neural network.\n",
    "\n",
    "        Parameters:\n",
    "            in_channels: int\n",
    "                The number of channels in the input image. For MNIST, this is 1 (grayscale images) but for mine, it is also 1.\n",
    "            num_classes: int\n",
    "                The number of classes we want to predict, in our case 3 (HAMS_C, HAMS_K, HAMS_Z).\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer: 2x2 window, stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Second convolutional layer: 8 input channels, 16 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # Third convolutional layer: 16 input channels, 32 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Fully connected layer: 32*16*16 input features (after three 2x2 poolings), 10 output features (num_classes)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, num_classes)\n",
    "        # add a dropout rate to prevent overfitting and co-adaptation\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x: torch.Tensor\n",
    "                The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "                The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = F.relu(self.conv3(x))  # Apply third convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = self.dropout(x)        # Apply dropout\n",
    "        x = self.fc1(x)            # Apply fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c252440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check accuracy on training and validation sets\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            scores = model(x)  # Get the output scores from the model\n",
    "            _, predictions = scores.max(1)  # Get the predicted class by finding the index of the max score\n",
    "            num_correct += (predictions == y).sum()  # Count how many predictions are correct\n",
    "            num_samples += predictions.size(0)  # Count the total number of samples\n",
    "    accuracy = float(num_correct) / num_samples * 100  # Calculate accuracy as a percentage\n",
    "    print(f\"Got {num_correct} / {num_samples} with accuracy {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17af72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "'''\n",
    "ImageFolder takes in a directory containing subdirectories, where each subdirectory corresponds to a class and contains images belonging to that class.\n",
    "The transform argument allows you to specify the transformations to apply to the images when they are loaded.\n",
    " In this case, we are converting the images to tensors and normalizing them with a mean of 0.5 and a standard deviation of 0.5.\n",
    "'''\n",
    "# Define the transformations to apply to the images\n",
    "# The transforms.Compose() function allows you to chain multiple transformations together. In this case, we are converting the images to grayscale, then to tensors, and finally normalizing them.\n",
    "# add various transormers for varioation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # forcing it to be grayscale, which means it will have only one channel. This is done to ensure that all images are in the same format and to reduce the computational complexity of the model.\n",
    "    transforms.RandomRotation(10), # Randomly rotate the images by up to 10 degrees. This is a data augmentation technique that helps the model generalize better by introducing some variation in the training data.\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # Randomly translate the images by up to 10% in both the x and y directions. This is another data augmentation technique that helps the model generalize better by introducing some variation in the training data.\n",
    "    transforms.ToTensor(), # Converting the images to tensors, which is the format that PyTorch models expect. This transformation also scales the pixel values to be between 0 and 1.\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalizing the pixel values to be between -1 and 1. The mean and std values are set to 0.5 to achieve this normalization.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf1c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the processed writer datasets\n",
    "WRITERS_DIR = Path(\"C:\\\\Users\\\\BC-Tech\\\\Documents\\\\Chibueze's Code\\\\Personal-Projects\\\\Handwriting-Distinguisher\\\\data\\\\processed\")\n",
    "\n",
    "# Attach the transform to the dataset\n",
    "dataset = ImageFolder(WRITERS_DIR, transform=transform)\n",
    "\n",
    "# Until this point, we have created a dataset object that can be used to load and preprocess the images. Now, we will create a DataLoader to load the data in batches and shuffle it for training.\n",
    "# You don't need to save the tensors anywhere\n",
    "# It only applies the loader when it is run and dones\\t permanently modify it\n",
    "# batchsize 16 only iterates through 16 random items in the dataset\n",
    "\n",
    "\"\"\"\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "images, labels = next(iter(loader)) # labels look like {'HAMS_C': 0, 'HAMS_K': 1, 'HAMS_Z': 2}\n",
    "\n",
    "print(f\"Batch of images shape: {images.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "# Spliting dataset into train/ validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0625aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epoch is one full pass through the training dataset\n",
    "model = CNN(in_channels=1, num_classes=3) # Initialize the model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4) # weight decay is a regularization technique that adds a penalty to the loss function based on the magnitude of the model's weights. This helps prevent overfitting by discouraging the model from relying too heavily on any particular feature or set of features.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870c4b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "418e5e35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4ef102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25]\n",
      "Loss: 1.0975\n",
      "Training Accuracy:\n",
      "Got 94 / 280 with accuracy 33.57%\n",
      "Validation Accuracy:\n",
      "Got 23 / 71 with accuracy 32.39%\n",
      "Epoch [2/25]\n",
      "Loss: 1.0983\n",
      "Training Accuracy:\n",
      "Got 96 / 280 with accuracy 34.29%\n",
      "Validation Accuracy:\n",
      "Got 23 / 71 with accuracy 32.39%\n",
      "Epoch [3/25]\n",
      "Loss: 1.0227\n",
      "Training Accuracy:\n",
      "Got 99 / 280 with accuracy 35.36%\n",
      "Validation Accuracy:\n",
      "Got 23 / 71 with accuracy 32.39%\n",
      "Epoch [4/25]\n",
      "Loss: 1.0403\n",
      "Training Accuracy:\n",
      "Got 122 / 280 with accuracy 43.57%\n",
      "Validation Accuracy:\n",
      "Got 31 / 71 with accuracy 43.66%\n",
      "Epoch [5/25]\n",
      "Loss: 1.0315\n",
      "Training Accuracy:\n",
      "Got 115 / 280 with accuracy 41.07%\n",
      "Validation Accuracy:\n",
      "Got 37 / 71 with accuracy 52.11%\n",
      "Epoch [6/25]\n",
      "Loss: 1.1863\n",
      "Training Accuracy:\n",
      "Got 156 / 280 with accuracy 55.71%\n",
      "Validation Accuracy:\n",
      "Got 39 / 71 with accuracy 54.93%\n",
      "Epoch [7/25]\n",
      "Loss: 1.0830\n",
      "Training Accuracy:\n",
      "Got 159 / 280 with accuracy 56.79%\n",
      "Validation Accuracy:\n",
      "Got 38 / 71 with accuracy 53.52%\n",
      "Epoch [8/25]\n",
      "Loss: 0.9488\n",
      "Training Accuracy:\n",
      "Got 166 / 280 with accuracy 59.29%\n",
      "Validation Accuracy:\n",
      "Got 35 / 71 with accuracy 49.30%\n",
      "Epoch [9/25]\n",
      "Loss: 1.2435\n",
      "Training Accuracy:\n",
      "Got 163 / 280 with accuracy 58.21%\n",
      "Validation Accuracy:\n",
      "Got 40 / 71 with accuracy 56.34%\n",
      "Epoch [10/25]\n",
      "Loss: 0.9080\n",
      "Training Accuracy:\n",
      "Got 162 / 280 with accuracy 57.86%\n",
      "Validation Accuracy:\n",
      "Got 42 / 71 with accuracy 59.15%\n",
      "Epoch [11/25]\n",
      "Loss: 0.7117\n",
      "Training Accuracy:\n",
      "Got 167 / 280 with accuracy 59.64%\n",
      "Validation Accuracy:\n",
      "Got 39 / 71 with accuracy 54.93%\n",
      "Epoch [12/25]\n",
      "Loss: 0.6446\n",
      "Training Accuracy:\n",
      "Got 162 / 280 with accuracy 57.86%\n",
      "Validation Accuracy:\n",
      "Got 37 / 71 with accuracy 52.11%\n",
      "Epoch [13/25]\n",
      "Loss: 0.7448\n",
      "Training Accuracy:\n",
      "Got 159 / 280 with accuracy 56.79%\n",
      "Validation Accuracy:\n",
      "Got 39 / 71 with accuracy 54.93%\n",
      "Epoch [14/25]\n",
      "Loss: 0.8992\n",
      "Training Accuracy:\n",
      "Got 157 / 280 with accuracy 56.07%\n",
      "Validation Accuracy:\n",
      "Got 39 / 71 with accuracy 54.93%\n",
      "Epoch [15/25]\n",
      "Loss: 0.9407\n",
      "Training Accuracy:\n",
      "Got 182 / 280 with accuracy 65.00%\n",
      "Validation Accuracy:\n",
      "Got 38 / 71 with accuracy 53.52%\n",
      "Epoch [16/25]\n",
      "Loss: 0.8712\n",
      "Training Accuracy:\n",
      "Got 178 / 280 with accuracy 63.57%\n",
      "Validation Accuracy:\n",
      "Got 38 / 71 with accuracy 53.52%\n",
      "Epoch [17/25]\n",
      "Loss: 0.5537\n",
      "Training Accuracy:\n",
      "Got 176 / 280 with accuracy 62.86%\n",
      "Validation Accuracy:\n",
      "Got 42 / 71 with accuracy 59.15%\n",
      "Epoch [18/25]\n",
      "Loss: 0.9475\n",
      "Training Accuracy:\n",
      "Got 170 / 280 with accuracy 60.71%\n",
      "Validation Accuracy:\n",
      "Got 42 / 71 with accuracy 59.15%\n",
      "Epoch [19/25]\n",
      "Loss: 0.8231\n",
      "Training Accuracy:\n",
      "Got 182 / 280 with accuracy 65.00%\n",
      "Validation Accuracy:\n",
      "Got 46 / 71 with accuracy 64.79%\n",
      "Epoch [20/25]\n",
      "Loss: 0.5098\n",
      "Training Accuracy:\n",
      "Got 173 / 280 with accuracy 61.79%\n",
      "Validation Accuracy:\n",
      "Got 44 / 71 with accuracy 61.97%\n",
      "Epoch [21/25]\n",
      "Loss: 0.4474\n",
      "Training Accuracy:\n",
      "Got 186 / 280 with accuracy 66.43%\n",
      "Validation Accuracy:\n",
      "Got 43 / 71 with accuracy 60.56%\n",
      "Epoch [22/25]\n",
      "Loss: 0.9412\n",
      "Training Accuracy:\n",
      "Got 186 / 280 with accuracy 66.43%\n",
      "Validation Accuracy:\n",
      "Got 39 / 71 with accuracy 54.93%\n",
      "Epoch [23/25]\n",
      "Loss: 0.9820\n",
      "Training Accuracy:\n",
      "Got 187 / 280 with accuracy 66.79%\n",
      "Validation Accuracy:\n",
      "Got 48 / 71 with accuracy 67.61%\n",
      "Epoch [24/25]\n",
      "Loss: 0.6132\n",
      "Training Accuracy:\n",
      "Got 188 / 280 with accuracy 67.14%\n",
      "Validation Accuracy:\n",
      "Got 47 / 71 with accuracy 66.20%\n",
      "Epoch [25/25]\n",
      "Loss: 0.4674\n",
      "Training Accuracy:\n",
      "Got 185 / 280 with accuracy 66.07%\n",
      "Validation Accuracy:\n",
      "Got 43 / 71 with accuracy 60.56%\n",
      "Smallest loss of 0.4474 occurred at epoch 21\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "min_loss = float('inf')\n",
    "smallest_loss_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    for images, labels in train_loader:\n",
    "        # Move data and targets to the device (GPU/CPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        scores = model(images)\n",
    "        loss = criterion(scores, labels)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step: update the model parameters\n",
    "        optimizer.step()\n",
    "    if loss.item() < min_loss or min_loss == float('inf'):\n",
    "        min_loss = loss.item()\n",
    "        smallest_loss_epoch = epoch\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    # Final accuracy check on training and test sets\n",
    "    print(\"Training Accuracy:\")\n",
    "    check_accuracy(train_loader, model)\n",
    "    print(\"Validation Accuracy:\")\n",
    "    check_accuracy(val_loader, model)\n",
    "print(f\"Smallest loss of {min_loss:.4f} occurred at epoch {smallest_loss_epoch + 1}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
